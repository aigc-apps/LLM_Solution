{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    "\n",
    "triton_client = httpclient.InferenceServerClient(url=\"127.0.0.1:7000\")\n",
    "\n",
    "\n",
    "def test_infer(\n",
    "    model_name,\n",
    "    input_ids__0,\n",
    "    attention_mask__1,\n",
    "    token_type_ids__2,\n",
    "    headers=None,\n",
    "    request_compression_algorithm=None,\n",
    "    response_compression_algorithm=None,\n",
    "):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    inputs.append(httpclient.InferInput(\"input_ids\", [1, 512], \"INT64\"))\n",
    "    inputs.append(httpclient.InferInput(\"attention_mask\", [1, 512], \"INT64\"))\n",
    "    inputs.append(httpclient.InferInput(\"token_type_ids\", [1, 512], \"INT64\"))\n",
    "\n",
    "    # Initialize the data\n",
    "    inputs[0].set_data_from_numpy(input_ids__0, binary_data=False)\n",
    "    inputs[1].set_data_from_numpy(attention_mask__1, binary_data=False)\n",
    "    inputs[2].set_data_from_numpy(token_type_ids__2, binary_data=False)\n",
    "\n",
    "    outputs.append(\n",
    "        httpclient.InferRequestedOutput(\"last_hidden_state\", binary_data=False)\n",
    "    )\n",
    "\n",
    "    results = triton_client.infer(\n",
    "        model_name,\n",
    "        inputs,\n",
    "        headers=headers,\n",
    "        request_compression_algorithm=request_compression_algorithm,\n",
    "        response_compression_algorithm=response_compression_algorithm,\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"/huggingface/sentence_transformers/bge-small-zh-v1.5-onnx/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.grpc.aio as grpcclient\n",
    "\n",
    "trion_grpc_client = grpcclient.InferenceServerClient(url=\"127.0.0.1:7001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_stats {\n",
       "  name: \"embedding\"\n",
       "  version: \"1\"\n",
       "  last_inference: 1720592176099\n",
       "  inference_count: 16254\n",
       "  execution_count: 16254\n",
       "  inference_stats {\n",
       "    success {\n",
       "      count: 16254\n",
       "      ns: 66613751697476\n",
       "    }\n",
       "    fail {\n",
       "    }\n",
       "    queue {\n",
       "      count: 16254\n",
       "      ns: 66576533712199\n",
       "    }\n",
       "    compute_input {\n",
       "      count: 16254\n",
       "      ns: 662801534\n",
       "    }\n",
       "    compute_infer {\n",
       "      count: 16254\n",
       "      ns: 30256391063\n",
       "    }\n",
       "    compute_output {\n",
       "      count: 16254\n",
       "      ns: 2323889201\n",
       "    }\n",
       "    cache_hit {\n",
       "    }\n",
       "    cache_miss {\n",
       "    }\n",
       "  }\n",
       "  batch_stats {\n",
       "    batch_size: 1\n",
       "    compute_input {\n",
       "      count: 16254\n",
       "      ns: 662801534\n",
       "    }\n",
       "    compute_infer {\n",
       "      count: 16254\n",
       "      ns: 30256391063\n",
       "    }\n",
       "    compute_output {\n",
       "      count: 16254\n",
       "      ns: 2323889201\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = await trion_grpc_client.get_inference_statistics()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3715036/1437513552.py:1: DeprecationWarning: module 'sre_parse' is deprecated\n",
      "  from sre_parse import FLAGS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.8712e-02,  1.1275e-01,  5.5719e-02,  3.1040e-02,  2.5992e-02,\n",
       "         -2.7015e-02,  1.2640e-02,  3.2104e-02,  3.8956e-02,  1.9829e-02,\n",
       "          5.6384e-02, -2.4350e-01, -2.2022e-02,  3.0023e-02, -6.0440e-02,\n",
       "          6.8122e-04, -4.3383e-02, -3.3204e-02, -2.5863e-02, -3.1667e-02,\n",
       "         -5.8114e-02, -4.7395e-02,  3.5730e-03, -1.9483e-02,  3.2852e-02,\n",
       "          1.4670e-02,  1.2862e-02, -6.6240e-02, -1.0591e-02, -3.6343e-02,\n",
       "         -1.0499e-04,  3.6151e-02,  4.2404e-02, -3.7466e-02,  1.4994e-02,\n",
       "         -8.7496e-02, -7.3626e-02,  1.8903e-03,  7.7163e-03,  7.2207e-02,\n",
       "         -9.2240e-03, -3.8684e-03, -2.6567e-03, -2.8499e-02, -1.3102e-02,\n",
       "          4.2263e-03,  1.8580e-02, -8.9796e-03, -8.3271e-03,  3.2754e-02,\n",
       "         -1.5525e-02,  9.7978e-03,  3.6212e-02, -1.5610e-02,  1.5459e-02,\n",
       "         -2.4204e-02,  7.4954e-03,  7.8971e-02,  2.0036e-03,  6.4317e-02,\n",
       "         -9.3554e-03, -1.1949e-02, -5.7218e-02, -3.8853e-02, -6.1532e-02,\n",
       "          1.4281e-02, -1.7761e-03,  3.6728e-04,  1.8937e-02,  5.2090e-02,\n",
       "          5.1332e-02,  3.4151e-02,  5.5599e-02, -2.1427e-02, -4.2273e-02,\n",
       "         -5.1176e-02,  2.6666e-02,  1.8626e-02,  1.9962e-02, -4.4103e-02,\n",
       "         -1.9542e-02, -3.1040e-02, -1.8725e-02,  5.8660e-02, -6.6289e-02,\n",
       "         -7.8842e-02, -2.1514e-03, -5.8642e-02, -9.2039e-03, -7.4694e-02,\n",
       "         -7.9614e-02, -8.7069e-02,  4.9292e-02, -3.1448e-02, -2.9198e-02,\n",
       "         -1.1257e-02,  5.5133e-03, -6.8082e-02, -5.0707e-02, -7.8087e-02,\n",
       "         -2.2787e-02, -2.6462e-02,  2.2279e-02,  1.5762e-02, -5.7909e-02,\n",
       "         -1.3021e-02,  2.5083e-02, -5.1394e-02,  1.5963e-02,  5.1788e-02,\n",
       "          4.4431e-02,  3.4066e-02,  3.3041e-02, -1.2254e-02, -4.0981e-02,\n",
       "         -5.4975e-02,  2.8179e-02,  1.9959e-02, -1.6637e-02,  2.6414e-02,\n",
       "         -2.0626e-02, -3.4108e-02,  1.3405e-02, -8.5190e-02, -5.7363e-02,\n",
       "         -3.9349e-03,  2.4087e-02,  8.2131e-04,  2.5457e-02, -1.4136e-02,\n",
       "         -1.5099e-02,  3.3015e-02,  4.4474e-02, -1.7246e-02, -9.3970e-02,\n",
       "          8.9218e-02, -3.3631e-02,  4.4339e-02, -2.4163e-02,  2.5809e-02,\n",
       "          1.1713e-02,  2.4888e-03,  8.5096e-03, -2.2238e-02,  8.4368e-03,\n",
       "         -2.2952e-02, -6.7363e-02,  8.7952e-02,  2.3705e-02, -9.8224e-03,\n",
       "          9.3768e-03, -3.7521e-02,  4.4408e-02,  5.8904e-02, -1.8442e-03,\n",
       "          1.8007e-02,  1.1447e-01,  3.0618e-02, -3.2920e-02, -4.8262e-02,\n",
       "          5.7300e-02,  6.7952e-03,  4.8323e-04, -6.5140e-02,  4.8650e-02,\n",
       "         -5.1574e-02,  3.1040e-02,  6.0686e-02, -1.7290e-02,  1.1144e-02,\n",
       "         -1.0672e-02,  2.7159e-02, -2.4772e-02, -5.0794e-02, -1.4627e-02,\n",
       "          3.7325e-02,  5.1350e-03,  7.4761e-02,  4.0030e-02, -1.0314e-02,\n",
       "          1.3036e-02,  3.5842e-02, -1.5036e-02, -7.7895e-03,  3.0688e-02,\n",
       "         -6.9159e-02, -1.2750e-01,  1.6110e-02, -5.3180e-02,  1.2508e-02,\n",
       "         -5.7345e-02,  4.8834e-02, -2.9282e-02, -6.1235e-02,  3.0168e-02,\n",
       "         -3.3901e-02, -3.8859e-02, -1.8805e-02,  4.8629e-02, -5.5203e-02,\n",
       "         -5.8495e-02, -1.5158e-02,  5.7647e-02,  4.5133e-02, -3.1789e-02,\n",
       "         -1.3637e-02, -7.3742e-02, -6.0757e-02,  2.3037e-02,  4.5425e-02,\n",
       "         -2.9480e-02, -1.3428e-02, -1.1751e-02,  2.2672e-02,  1.0128e-02,\n",
       "          5.5125e-02,  5.0928e-02,  3.2210e-03,  2.4906e-02, -9.0012e-02,\n",
       "          2.8754e-02,  2.7522e-02,  2.9744e-02, -3.0885e-02, -9.3507e-03,\n",
       "          6.6700e-04,  4.7634e-02, -2.6753e-02,  8.8103e-03, -2.3501e-03,\n",
       "         -6.6476e-03, -3.4261e-02, -8.5463e-03, -5.8365e-02,  4.4506e-02,\n",
       "         -2.4853e-02, -3.1707e-02,  7.5738e-02,  5.7346e-02, -5.6313e-02,\n",
       "          5.0012e-03,  8.9037e-02, -2.8646e-02, -2.1305e-02,  7.9669e-03,\n",
       "         -4.4562e-02, -1.1368e-01, -2.4425e-02,  2.9928e-02, -3.2894e-03,\n",
       "         -3.7323e-02,  5.9222e-03, -1.9304e-02, -5.2070e-02, -7.2788e-02,\n",
       "         -3.9160e-02, -3.2153e-02, -2.3582e-02,  4.1526e-03,  2.2560e-03,\n",
       "          1.5677e-02, -3.4313e-03,  1.4803e-02,  9.0704e-03, -2.4546e-02,\n",
       "         -1.0685e-02, -4.7826e-02,  7.8357e-02,  1.2904e-02, -3.0078e-02,\n",
       "          7.9362e-02,  6.4407e-02,  8.9200e-03, -4.2975e-02, -6.6931e-03,\n",
       "          2.6380e-02, -4.0144e-02,  6.8500e-02, -4.1489e-02, -4.7009e-02,\n",
       "         -2.7699e-02,  1.3624e-02, -1.2040e-02,  3.3126e-02,  4.1210e-03,\n",
       "          4.7026e-02, -5.3973e-02, -1.5932e-02,  9.0921e-04,  3.2000e-02,\n",
       "          4.0989e-02, -3.2288e-03,  3.4434e-02,  2.5334e-02, -5.3949e-02,\n",
       "         -6.1343e-02, -2.9357e-02,  1.0650e-02,  4.6326e-02, -5.4464e-02,\n",
       "         -1.5081e-02, -2.7592e-02, -3.1407e-02,  2.3281e-03,  2.3484e-02,\n",
       "         -3.1990e-02, -1.1015e-02, -3.8523e-02,  5.0830e-02, -5.6357e-03,\n",
       "          2.3382e-02, -7.5032e-02,  2.6172e-02, -5.1325e-02,  3.7987e-02,\n",
       "         -3.0229e-02, -1.1208e-03, -3.1397e-02, -3.3557e-02,  3.4050e-02,\n",
       "          5.9109e-02,  2.1414e-02,  5.4880e-02, -1.0372e-02,  1.0622e-01,\n",
       "         -3.8704e-02,  1.3552e-02,  7.3380e-02,  4.4216e-02, -6.0973e-02,\n",
       "          5.1187e-02,  2.8064e-02, -1.7767e-02,  5.9838e-02,  5.3850e-02,\n",
       "         -6.3004e-02, -1.8858e-02, -2.2803e-02,  4.0287e-02,  4.5410e-02,\n",
       "         -5.1616e-03,  1.7615e-02,  1.5218e-02,  3.3759e-02, -3.1507e-02,\n",
       "         -5.8321e-03, -3.8346e-02, -1.3905e-03,  5.5830e-02,  7.8915e-02,\n",
       "          3.2141e-02,  8.6827e-03,  5.8391e-02,  1.2820e-02, -5.0135e-03,\n",
       "         -1.0051e-01, -1.1925e-02, -2.2768e-02,  3.7689e-03,  1.2874e-02,\n",
       "         -6.2870e-02, -8.1522e-02, -6.6079e-02,  1.0492e-01,  2.7221e-02,\n",
       "         -4.1626e-02, -1.1903e-02, -2.8072e-02, -4.3979e-02,  9.6114e-04,\n",
       "          8.9349e-03,  2.6634e-02,  4.4191e-02, -3.4690e-02,  6.7881e-02,\n",
       "         -2.5428e-02, -1.0186e-02,  5.3219e-02,  3.4281e-02,  2.9126e-02,\n",
       "         -4.8661e-02,  2.3944e-02,  2.6782e-02,  1.2571e-02,  1.4401e-01,\n",
       "         -4.1819e-02, -7.4408e-02, -4.5223e-02,  3.3863e-02,  8.1997e-02,\n",
       "          1.0251e-01, -1.1713e-02,  7.9623e-03,  9.1189e-03,  2.1536e-02,\n",
       "         -1.6640e-02, -3.4294e-03,  3.4197e-02,  6.9057e-03,  1.8021e-02,\n",
       "          2.7275e-02, -4.4101e-03, -5.0859e-02,  4.4284e-02, -6.2234e-03,\n",
       "          3.5889e-02, -4.0410e-02,  1.1971e-02,  2.5538e-02,  1.1542e-02,\n",
       "          2.1355e-01, -6.3060e-02, -5.2475e-02, -3.3767e-02,  1.1064e-02,\n",
       "          8.0536e-02, -5.4242e-02,  2.0165e-02,  1.5035e-02, -3.4339e-04,\n",
       "          2.0263e-02,  2.0582e-02,  1.7606e-02, -1.5463e-02,  2.8840e-03,\n",
       "         -4.0691e-02, -2.1328e-02,  7.4539e-03,  1.7937e-02, -1.5746e-02,\n",
       "          2.6958e-02,  4.1410e-02,  5.4245e-03,  3.5160e-02, -3.9476e-02,\n",
       "          7.1817e-02, -7.5571e-02,  2.2223e-02, -3.0715e-02,  4.8774e-02,\n",
       "          6.5908e-04,  4.2919e-02, -2.2606e-02, -5.6193e-03, -1.9210e-02,\n",
       "         -8.8266e-03,  6.2167e-02, -8.2328e-04, -7.9505e-02, -2.5662e-02,\n",
       "          1.9761e-02,  5.0226e-03,  8.8631e-02, -5.2292e-02, -7.3979e-03,\n",
       "          3.3663e-02, -4.0846e-02, -4.8236e-02, -1.5163e-02, -2.3082e-02,\n",
       "         -2.8543e-02, -1.1158e-02, -2.1625e-02,  7.2827e-03, -6.7261e-02,\n",
       "          1.0293e-03, -2.4227e-02, -7.1658e-02,  4.1967e-02, -3.0592e-02,\n",
       "         -1.8500e-02, -1.2488e-02, -4.6488e-02, -1.8684e-02,  2.9397e-02,\n",
       "          3.4058e-02, -1.9876e-02,  5.2594e-03, -6.2573e-03, -1.7652e-02,\n",
       "         -2.7034e-02, -2.1101e-02, -3.8882e-03,  3.5602e-02,  4.1351e-02,\n",
       "          1.2260e-01,  5.9450e-02,  1.5567e-02,  6.5519e-04, -6.9907e-02,\n",
       "         -1.1880e-01, -3.5195e-02, -6.4135e-03, -9.8884e-02,  1.4048e-02,\n",
       "         -2.6009e-02,  1.9171e-02, -6.8398e-02,  5.1199e-03,  5.3149e-02,\n",
       "         -4.9032e-02,  2.9342e-02,  3.6905e-02, -1.1500e-02, -1.3884e-02,\n",
       "         -5.9570e-02, -4.2575e-02,  1.3308e-02, -2.5794e-02, -3.4755e-02,\n",
       "          5.9679e-02,  8.1808e-04]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sre_parse import FLAGS\n",
    "\n",
    "\n",
    "async def test_infer_async(\n",
    "    model_name, input_ids__0, attention_mask__1, token_type_ids__2, headers=None\n",
    "):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    inputs.append(grpcclient.InferInput(\"input_ids\", [1, 512], \"INT64\"))\n",
    "    inputs.append(grpcclient.InferInput(\"attention_mask\", [1, 512], \"INT64\"))\n",
    "    inputs.append(grpcclient.InferInput(\"token_type_ids\", [1, 512], \"INT64\"))\n",
    "\n",
    "    # Initialize the data\n",
    "    inputs[0].set_data_from_numpy(input_ids__0)\n",
    "    inputs[1].set_data_from_numpy(attention_mask__1)\n",
    "    inputs[2].set_data_from_numpy(token_type_ids__2)\n",
    "\n",
    "    outputs.append(grpcclient.InferRequestedOutput(\"last_hidden_state\"))\n",
    "\n",
    "    results = await trion_grpc_client.infer(\n",
    "        model_name,\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        headers=headers,\n",
    "        compression_algorithm=\"gzip\",\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [\"我爱北京天安门\"],\n",
    "    is_split_into_words=False,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    max_length=512,\n",
    ")\n",
    "\n",
    "results = await test_infer_async(\n",
    "    \"embedding\",\n",
    "    inputs[\"input_ids\"].numpy(),\n",
    "    inputs[\"attention_mask\"].numpy(),\n",
    "    inputs[\"token_type_ids\"].numpy(),\n",
    ")\n",
    "\n",
    "model_output = torch.from_numpy(results.as_numpy(\"last_hidden_state\"))\n",
    "embeddings = model_output[:, 0]\n",
    "embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed:  1.4345321655273438\n",
      "elapsed:  12.041364431381226\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "tasks = []\n",
    "for i in range(5000):\n",
    "    inputs = tokenizer(\n",
    "        [\"我爱北京天安门\"],\n",
    "        is_split_into_words=False,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "    tasks.append(\n",
    "        test_infer_async(\n",
    "            \"embedding\",\n",
    "            inputs[\"input_ids\"].numpy(),\n",
    "            inputs[\"attention_mask\"].numpy(),\n",
    "            inputs[\"token_type_ids\"].numpy(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"elapsed: \", time.time() - start)\n",
    "results = await asyncio.gather(*tasks)\n",
    "print(\"elapsed: \", time.time() - start)\n",
    "\n",
    "embedding_list = []\n",
    "for result in results:\n",
    "    model_output = torch.from_numpy(result.as_numpy(\"last_hidden_state\"))\n",
    "    embeddings = model_output[:, 0]\n",
    "    embedding_list.append(torch.nn.functional.normalize(embeddings, p=2, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:  ['input_ids', 'token_type_ids', 'attention_mask']\n",
      "Outputs:  ['last_hidden_state']\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "model = onnx.load(\n",
    "    \"/huggingface/sentence_transformers/bge-small-zh-v1.5-onnx/model.onnx\"\n",
    ")\n",
    "output = [node.name for node in model.graph.output]\n",
    "\n",
    "input_all = [node.name for node in model.graph.input]\n",
    "input_initializer = [node.name for node in model.graph.initializer]\n",
    "net_feed_input = list(set(input_all) - set(input_initializer))\n",
    "\n",
    "print(\"Inputs: \", net_feed_input)\n",
    "print(\"Outputs: \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/huggingface/sentence_transformers/bge-small-zh-v1.5-onnx/model.onnx\n",
      "False\n",
      "<onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions object at 0x7f978eede2f0>\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f978eede2f0>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f978eedef30>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f978eede7b0>]\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f96819311b0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-07-10 15:12:47.787512132 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 4 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2024-07-10 15:12:47.788369070 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-07-10 15:12:47.788377674 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "from pai_rag.modules.embedding.my_ort_embedding import MyORTModelForFeatureExtraction\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "onnx_path = \"/huggingface/sentence_transformers/bge-small-zh-v1.5-onnx\"\n",
    "model = MyORTModelForFeatureExtraction.from_pretrained(\n",
    "    onnx_path, file_name=\"model.onnx\", provider=\"CUDAExecutionProvider\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(onnx_path)\n",
    "max_length = model.config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5000):\n",
    "    text = \"我爱北京天安门\"\n",
    "    encoded_input = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    model_output = model(**encoded_input)\n",
    "    embeddings = model_output[0][:, 0]\n",
    "\n",
    "    embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onnx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
