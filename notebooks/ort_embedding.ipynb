{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Using cached huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting numpy<2.0,>=1.17 (from transformers)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Using cached tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
      "Using cached huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "Using cached PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
      "Using cached regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "Using cached safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n",
      "Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: urllib3, tqdm, safetensors, regex, pyyaml, numpy, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed certifi-2024.7.4 charset-normalizer-3.3.2 filelock-3.15.4 fsspec-2024.6.1 huggingface-hub-0.23.4 idna-3.7 numpy-1.26.4 pyyaml-6.0.1 regex-2024.5.15 requests-2.32.3 safetensors-0.4.3 tokenizers-0.19.1 tqdm-4.66.4 transformers-4.42.3 urllib3-2.2.2\n",
      "Collecting llama-index-embeddings-huggingface-optimum\n",
      "  Using cached llama_index_embeddings_huggingface_optimum-0.1.5-py3-none-any.whl.metadata (761 bytes)\n",
      "Collecting llama-index-core<0.11.0,>=0.10.1 (from llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached llama_index_core-0.10.52.post1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-embeddings-huggingface<0.2.0,>=0.1.3 (from llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached llama_index_embeddings_huggingface-0.1.5-py3-none-any.whl.metadata (755 bytes)\n",
      "Collecting optimum<2.0.0,>=1.16.2 (from optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached optimum-1.21.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (6.0.1)\n",
      "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached SQLAlchemy-2.0.31-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.6 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached aiohttp-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (2024.6.1)\n",
      "Collecting httpx (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting llama-cloud<0.0.7,>=0.0.6 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached llama_cloud-0.0.6-py3-none-any.whl.metadata (750 bytes)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (1.6.0)\n",
      "Collecting networkx>=3.0 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting nltk<4.0.0,>=3.8.1 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy<2.0.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (1.26.4)\n",
      "Collecting openai>=1.1.0 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached openai-1.35.10-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting pandas (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting pillow>=9.0.0 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: requests>=2.31.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached tenacity-8.4.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (4.12.2)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum) (0.23.4)\n",
      "Collecting torch<3.0.0,>=2.1.2 (from llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached torch-2.3.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.37.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum) (4.42.3)\n",
      "Collecting coloredlogs (from optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting sympy (from optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached sympy-1.12.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum) (24.1)\n",
      "Collecting datasets (from optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting onnx (from optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached onnx-1.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting onnxruntime (from optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached onnxruntime-1.18.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting timm (from optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached timm-1.0.7-py3-none-any.whl.metadata (47 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached multidict-6.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: filelock in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum) (3.15.4)\n",
      "Collecting minijinja>=1.0 (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached minijinja-2.0.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting pydantic>=1.10 (from llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting anyio (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: certifi in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (2024.7.4)\n",
      "Collecting httpcore==1.* (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: idna in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (3.7)\n",
      "Collecting sniffio (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting click (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (2024.5.15)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (2.2.2)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached greenlet-3.0.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting jinja2 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.3.1 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached triton-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers<5.0.0,>=4.37.0->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers<5.0.0,>=4.37.0->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum) (0.19.1)\n",
      "Collecting protobuf (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached protobuf-5.27.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached pyarrow-16.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xxhash (from datasets->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached xxhash-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting flatbuffers (from onnxruntime->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (2.9.0)\n",
      "Collecting pytz>=2020.1 (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting mpmath<1.4.0,>=1.1.0 (from sympy->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting torchvision (from timm->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached torchvision-0.18.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached pydantic_core-2.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (1.16.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Using cached llama_index_embeddings_huggingface_optimum-0.1.5-py3-none-any.whl (3.7 kB)\n",
      "Using cached llama_index_core-0.10.52.post1-py3-none-any.whl (15.4 MB)\n",
      "Using cached llama_index_embeddings_huggingface-0.1.5-py3-none-any.whl (7.8 kB)\n",
      "Using cached optimum-1.21.1-py3-none-any.whl (424 kB)\n",
      "Using cached aiohttp-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Using cached llama_cloud-0.0.6-py3-none-any.whl (130 kB)\n",
      "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached openai-1.35.10-py3-none-any.whl (328 kB)\n",
      "Using cached pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Using cached SQLAlchemy-2.0.31-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached tenacity-8.4.2-py3-none-any.whl (28 kB)\n",
      "Using cached tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Using cached torch-2.3.1-cp311-cp311-manylinux1_x86_64.whl (779.2 MB)\n",
      "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Using cached triton-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "Using cached fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "Using cached onnx-1.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
      "Using cached onnxruntime-1.18.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
      "Using cached pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "Using cached sympy-1.12.1-py3-none-any.whl (5.7 MB)\n",
      "Using cached timm-1.0.7-py3-none-any.whl (2.3 MB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)\n",
      "Using cached greenlet-3.0.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (620 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "Using cached minijinja-2.0.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (853 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached multidict-6.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached protobuf-5.27.2-cp38-abi3-manylinux2014_x86_64.whl (309 kB)\n",
      "Using cached pyarrow-16.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "Downloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.9/423.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pydantic_core-2.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (328 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached torchvision-0.18.1-cp311-cp311-manylinux1_x86_64.whl (7.0 MB)\n",
      "Using cached xxhash-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
      "Installing collected packages: sentencepiece, pytz, mpmath, flatbuffers, dirtyjson, xxhash, wrapt, tzdata, triton, tenacity, sympy, sniffio, pydantic-core, pyarrow-hotfix, pyarrow, protobuf, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, mypy-extensions, multidict, minijinja, marshmallow, MarkupSafe, joblib, humanfriendly, h11, greenlet, fsspec, frozenlist, distro, dill, click, attrs, annotated-types, yarl, typing-inspect, tiktoken, SQLAlchemy, pydantic, pandas, onnx, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nltk, multiprocess, jinja2, httpcore, deprecated, coloredlogs, anyio, aiosignal, onnxruntime, nvidia-cusolver-cu12, httpx, dataclasses-json, aiohttp, torch, openai, llama-cloud, torchvision, llama-index-core, datasets, timm, optimum, llama-index-embeddings-huggingface, llama-index-embeddings-huggingface-optimum\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.1\n",
      "    Uninstalling fsspec-2024.6.1:\n",
      "      Successfully uninstalled fsspec-2024.6.1\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install llama-index-embeddings-huggingface-optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optimum in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (1.21.1)\n",
      "Requirement already satisfied: coloredlogs in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from optimum) (15.0.1)\n",
      "Requirement already satisfied: sympy in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from optimum) (1.12.1)\n",
      "Requirement already satisfied: transformers<4.43.0,>=4.26.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (4.42.3)\n",
      "Requirement already satisfied: torch>=1.11 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from optimum) (2.3.1)\n",
      "Requirement already satisfied: packaging in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from optimum) (24.1)\n",
      "Requirement already satisfied: numpy<2.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from optimum) (1.26.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from optimum) (0.23.4)\n",
      "Requirement already satisfied: datasets in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from optimum) (2.20.0)\n",
      "Requirement already satisfied: filelock in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub>=0.8.0->optimum) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub>=0.8.0->optimum) (6.0.1)\n",
      "Requirement already satisfied: requests in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub>=0.8.0->optimum) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub>=0.8.0->optimum) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub>=0.8.0->optimum) (4.12.2)\n",
      "Requirement already satisfied: networkx in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (3.3)\n",
      "Requirement already satisfied: jinja2 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->optimum) (12.5.82)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (2024.5.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.19.1)\n",
      "Requirement already satisfied: protobuf in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (5.27.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.2.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from coloredlogs->optimum) (10.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from datasets->optimum) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from datasets->optimum) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from datasets->optimum) (0.3.8)\n",
      "Requirement already satisfied: pandas in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from datasets->optimum) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from datasets->optimum) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from datasets->optimum) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from datasets->optimum) (3.9.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from sympy->optimum) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from aiohttp->datasets->optimum) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from aiohttp->datasets->optimum) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from aiohttp->datasets->optimum) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from aiohttp->datasets->optimum) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from jinja2->torch>=1.11->optimum) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from pandas->datasets->optimum) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from pandas->datasets->optimum) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from pandas->datasets->optimum) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\n",
      "Requirement already satisfied: onnx in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (1.16.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from onnx) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from onnx) (5.27.2)\n",
      "Looking in indexes: https://pypi.org/simple, https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n",
      "Requirement already satisfied: onnxruntime-gpu in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (1.18.1)\n",
      "Requirement already satisfied: coloredlogs in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from onnxruntime-gpu) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from onnxruntime-gpu) (24.3.25)\n",
      "Requirement already satisfied: numpy<2.0,>=1.21.6 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from onnxruntime-gpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from onnxruntime-gpu) (24.1)\n",
      "Requirement already satisfied: protobuf in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from onnxruntime-gpu) (5.27.2)\n",
      "Requirement already satisfied: sympy in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from onnxruntime-gpu) (1.12.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from coloredlogs->onnxruntime-gpu) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from sympy->onnxruntime-gpu) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install optimum\n",
    "!pip install onnx\n",
    "!pip install onnxruntime-gpu --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/huggingface/sentence_transformers/bge-small-zh-v1.5'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "from transformers import Pipeline\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "model_dir = \"/huggingface/sentence_transformers\"\n",
    "model_name = \"bge-small-zh-v1.5\"\n",
    "\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `from_transformers` is deprecated, and will be removed in optimum 2.0.  Use `export` instead\n",
      "Framework not specified. Using pt to export the model.\n",
      "Using the export variant default. Available variants are:\n",
      "    - default: The default ONNX variant.\n",
      "\n",
      "***** Exporting submodel 1/1: BertModel *****\n",
      "Using framework PyTorch: 2.3.1+cu121\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmprey_qwo1/model.onnx\n",
      "False\n",
      "<onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions object at 0x7fdcbf415370>\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdcbf415370>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdcbf415770>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdcc0a0baf0>]\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdcbf415370>]\n",
      "/tmp/tmprey_qwo1/model.onnx\n",
      "False\n",
      "<onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions object at 0x7fdcb9672ab0>\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdcb9672ab0>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdcb9672b30>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdcb9672bb0>]\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdcb9672ab0>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./tmp/bge_onnx/tokenizer_config.json',\n",
       " './tmp/bge_onnx/special_tokens_map.json',\n",
       " './tmp/bge_onnx/vocab.txt',\n",
       " './tmp/bge_onnx/added_tokens.json',\n",
       " './tmp/bge_onnx/tokenizer.json')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_path = \"./tmp/bge_onnx\"\n",
    "# load vanilla transformers and convert to onnx\n",
    "model = ORTModelForFeatureExtraction.from_pretrained(model_path, from_transformers=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# save onnx checkpoint and tokenizer\n",
    "model.save_pretrained(onnx_path)\n",
    "tokenizer.save_pretrained(onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "use_io_binding was set to False, setting it to True because it can provide a huge speedup on GPUs. It is possible to disable this feature manually by setting the use_io_binding attribute back to False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmprey_qwo1/model.onnx\n",
      "False\n",
      "<onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions object at 0x7fdcb9672ab0>\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdcb9672ab0>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdcb96737b0>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdcc015e5f0>]\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdcb9672ab0>]\n",
      "tensor([ 0.0472,  0.0101,  0.0006, -0.0082, -0.0220])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-07-06 11:32:52.479354501 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 4 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2024-07-06 11:32:52.480349243 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-07-06 11:32:52.480358848 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# copied from the model card\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[\n",
    "        0\n",
    "    ]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = (\n",
    "        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    )\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "        input_mask_expanded.sum(1), min=1e-9\n",
    "    )\n",
    "\n",
    "\n",
    "class SentenceEmbeddingPipeline(Pipeline):\n",
    "    def _sanitize_parameters(self, **kwargs):\n",
    "        # we don't have any hyperameters to sanitize\n",
    "        preprocess_kwargs = {}\n",
    "        return preprocess_kwargs, {}, {}\n",
    "\n",
    "    def preprocess(self, inputs):\n",
    "        encoded_inputs = self.tokenizer(\n",
    "            inputs, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        return encoded_inputs\n",
    "\n",
    "    def _forward(self, model_inputs):\n",
    "        outputs = self.model(**model_inputs)\n",
    "        return {\"outputs\": outputs, \"attention_mask\": model_inputs[\"attention_mask\"]}\n",
    "\n",
    "    def postprocess(self, model_outputs):\n",
    "        # Perform pooling\n",
    "        sentence_embeddings = mean_pooling(\n",
    "            model_outputs[\"outputs\"], model_outputs[\"attention_mask\"]\n",
    "        )\n",
    "        # Normalize embeddings\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        return sentence_embeddings\n",
    "\n",
    "\n",
    "vanilla_emb = SentenceEmbeddingPipeline(model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# run inference\n",
    "pred = vanilla_emb(\"Could you assist me in finding my lost card?\")\n",
    "\n",
    "# print an excerpt from the sentence embedding\n",
    "print(pred[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages/optimum/onnxruntime/configuration.py:779: FutureWarning: disable_embed_layer_norm will be deprecated soon, use disable_embed_layer_norm_fusion instead, disable_embed_layer_norm_fusion is set to True.\n",
      "  warnings.warn(\n",
      "Optimizing model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/bge_onnx/model.onnx\n",
      "False\n",
      "<onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions object at 0x7fdc4d9a0b70>\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdc4d9a0b70>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdc4d9a2a70>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdc4d9a38f0>]\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdc4d9a0b70>]\n",
      "tmp/bge_onnx/model.onnx\n",
      "False\n",
      "<onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions object at 0x7fdc579d5f70>\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdc579d6870>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdc579d5eb0>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdc579d4ab0>]\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdc579d6870>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-07-06 11:43:44.402180201 [W:onnxruntime:, inference_session.cc:1978 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.\u001b[m\n",
      "Configuration saved in tmp/bge_onnx/ort_config.json\n",
      "Optimized model saved at: tmp/bge_onnx (external data format: False; saved all tensor to one file: True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('tmp/bge_onnx')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTOptimizer\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig\n",
    "\n",
    "model = ORTModelForFeatureExtraction.from_pretrained(onnx_path, file_name=\"model.onnx\")\n",
    "\n",
    "# create ORTOptimizer and define optimization configuration\n",
    "optimizer = ORTOptimizer.from_pretrained(model)\n",
    "optimization_config = OptimizationConfig(\n",
    "    optimization_level=99\n",
    ")  # enable all optimizations\n",
    "\n",
    "# apply the optimization configuration to the model\n",
    "optimizer.optimize(\n",
    "    save_dir=onnx_path,\n",
    "    optimization_config=optimization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "use_io_binding was set to False, setting it to True because it can provide a huge speedup on GPUs. It is possible to disable this feature manually by setting the use_io_binding attribute back to False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/bge_onnx/model_optimized.onnx\n",
      "False\n",
      "<onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions object at 0x7fdcbcebcff0>\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdcbcebcff0>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdcbcebcab0>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdcb9687f70>]\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdcbcebcff0>]\n",
      "tmp/bge_onnx/model_optimized.onnx\n",
      "False\n",
      "<onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions object at 0x7fdc57c34370>\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdc57c34370>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdc57c34230>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdc57c34530>]\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdc57c34370>]\n",
      "tensor([ 0.0472,  0.0101,  0.0006, -0.0082, -0.0220])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-07-06 11:43:54.030874495 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 4 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2024-07-06 11:43:54.031722394 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-07-06 11:43:54.031731398 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
    "\n",
    "# load optimized model\n",
    "model = ORTModelForFeatureExtraction.from_pretrained(\n",
    "    onnx_path, file_name=\"model_optimized.onnx\"\n",
    ")\n",
    "\n",
    "# create optimized pipeline\n",
    "optimized_emb = SentenceEmbeddingPipeline(model=model, tokenizer=tokenizer, device=0)\n",
    "pred = optimized_emb(\"Could you assist me in finding my lost card?\")\n",
    "print(pred[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply ORTQuantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating dynamic quantizer: QOperator (mode: IntegerOps, schema: u8/s8, channel-wise: False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/bge_onnx/model_optimized.onnx\n",
      "False\n",
      "<onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions object at 0x7fdcbce62df0>\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdc758801b0>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdcbce62df0>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdc4f4c9330>]\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdc758801b0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing model...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unable to find data type for weight_name='/encoder/layer.0/attention/self/query/MatMul_output_0'. shape_inference failed to return a type probably this node is from a different domain or using an input produced by such an operator. This may happen if you quantize a model already quantized. You may use extra_options `DefaultTensorType` to indicate the default weight type, usually `onnx.TensorProto.FLOAT`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m dqconfig \u001b[38;5;241m=\u001b[39m AutoQuantizationConfig\u001b[38;5;241m.\u001b[39mavx512_vnni(is_static\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, per_channel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# apply the quantization configuration to the model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model_quantized_path \u001b[38;5;241m=\u001b[39m \u001b[43mdynamic_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monnx_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdqconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages/optimum/onnxruntime/quantization.py:401\u001b[0m, in \u001b[0;36mORTQuantizer.quantize\u001b[0;34m(self, quantization_config, save_dir, file_suffix, calibration_tensors_range, use_external_data_format, preprocessor)\u001b[0m\n\u001b[1;32m    398\u001b[0m quantizer \u001b[38;5;241m=\u001b[39m quantizer_factory(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mquantizer_kwargs)\n\u001b[1;32m    400\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuantizing model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 401\u001b[0m \u001b[43mquantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m suffix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_suffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file_suffix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    404\u001b[0m quantized_model_path \u001b[38;5;241m=\u001b[39m save_dir\u001b[38;5;241m.\u001b[39mjoinpath(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monnx_model_path\u001b[38;5;241m.\u001b[39mstem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mwith_suffix(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages/onnxruntime/quantization/onnx_quantizer.py:211\u001b[0m, in \u001b[0;36mONNXQuantizer.quantize_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m number_of_existing_new_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_nodes)\n\u001b[1;32m    210\u001b[0m op_quantizer \u001b[38;5;241m=\u001b[39m CreateOpQuantizer(\u001b[38;5;28mself\u001b[39m, node)\n\u001b[0;32m--> 211\u001b[0m \u001b[43mop_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(number_of_existing_new_nodes, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_nodes)):\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_nodes[i]\u001b[38;5;241m.\u001b[39moutput:\n",
      "File \u001b[0;32m/mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages/onnxruntime/quantization/operators/matmul.py:78\u001b[0m, in \u001b[0;36mMatMulInteger.quantize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Add cast operation to cast matmulInteger output to float.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m cast_op_output \u001b[38;5;241m=\u001b[39m matmul_integer_output \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cast_output\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 78\u001b[0m otype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tensor_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmandatory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m cast_node \u001b[38;5;241m=\u001b[39m onnx\u001b[38;5;241m.\u001b[39mhelper\u001b[38;5;241m.\u001b[39mmake_node(\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCast\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     81\u001b[0m     [matmul_integer_output],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m     to\u001b[38;5;241m=\u001b[39motype,\n\u001b[1;32m     85\u001b[0m )\n\u001b[1;32m     86\u001b[0m nodes\u001b[38;5;241m.\u001b[39mappend(cast_node)\n",
      "File \u001b[0;32m/mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages/onnxruntime/quantization/onnx_quantizer.py:271\u001b[0m, in \u001b[0;36mONNXQuantizer.get_tensor_type\u001b[0;34m(self, tensor_name, mandatory)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_subgraph_quantization) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mandatory:\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_default_tensor_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    273\u001b[0m otype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mis_valid_quantize_weight(tensor_name)\n",
      "File \u001b[0;32m/mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages/onnxruntime/quantization/onnx_quantizer.py:250\u001b[0m, in \u001b[0;36mONNXQuantizer._get_default_tensor_type\u001b[0;34m(self, tensor_name)\u001b[0m\n\u001b[1;32m    244\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_tensor_type returns DefaultTensorType for tensor name \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m, use \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    246\u001b[0m         tensor_name,\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefaultTensorType\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    248\u001b[0m     )\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefaultTensorType\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find data type for weight_name=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape_inference failed to return a type probably this node is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom a different domain or using an input produced by such an operator. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis may happen if you quantize a model already quantized. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou may use extra_options `DefaultTensorType` to indicate \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe default weight type, usually `onnx.TensorProto.FLOAT`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to find data type for weight_name='/encoder/layer.0/attention/self/query/MatMul_output_0'. shape_inference failed to return a type probably this node is from a different domain or using an input produced by such an operator. This may happen if you quantize a model already quantized. You may use extra_options `DefaultTensorType` to indicate the default weight type, usually `onnx.TensorProto.FLOAT`."
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "\n",
    "model = ORTModelForFeatureExtraction.from_pretrained(\n",
    "    onnx_path, file_name=\"model_optimized.onnx\"\n",
    ")\n",
    "\n",
    "# create ORTQuantizer and define quantization configuration\n",
    "dynamic_quantizer = ORTQuantizer.from_pretrained(model)\n",
    "dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)\n",
    "\n",
    "# apply the quantization configuration to the model\n",
    "model_quantized_path = dynamic_quantizer.quantize(\n",
    "    save_dir=onnx_path,\n",
    "    quantization_config=dqconfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import TYPE_CHECKING, Any, Dict, List, Optional, Set, Tuple, Union\n",
    "from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "\n",
    "\n",
    "class MyORTModelForFeatureExtraction(ORTModelForFeatureExtraction):\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[Union[torch.Tensor, np.ndarray]] = None,\n",
    "        attention_mask: Optional[Union[torch.Tensor, np.ndarray]] = None,\n",
    "        token_type_ids: Optional[Union[torch.Tensor, np.ndarray]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        use_torch = isinstance(input_ids, torch.Tensor)\n",
    "        self.raise_on_numpy_input_io_binding(use_torch)\n",
    "\n",
    "        if self.device.type == \"cuda\" and self.use_io_binding:\n",
    "            io_binding, output_shapes, output_buffers = self.prepare_io_binding(\n",
    "                input_ids,\n",
    "                attention_mask,\n",
    "                token_type_ids,\n",
    "                ordered_input_names=self._ordered_input_names,\n",
    "            )\n",
    "\n",
    "            # run inference with binding & synchronize in case of multiple CUDA streams\n",
    "            io_binding.synchronize_inputs()\n",
    "            self.model.run_with_iobinding(io_binding)\n",
    "            io_binding.synchronize_outputs()\n",
    "\n",
    "            last_hidden_state = output_buffers[\"last_hidden_state\"].view(\n",
    "                output_shapes[\"last_hidden_state\"]\n",
    "            )\n",
    "        else:\n",
    "            model_inputs = {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"token_type_ids\": token_type_ids,\n",
    "            }\n",
    "\n",
    "            onnx_inputs = self._prepare_onnx_inputs(use_torch, **model_inputs)\n",
    "            onnx_outputs = self.model.run(None, onnx_inputs)\n",
    "            model_outputs = self._prepare_onnx_outputs(use_torch, *onnx_outputs)\n",
    "\n",
    "            if \"last_hidden_state\" in self.output_names:\n",
    "                last_hidden_state = model_outputs[\"last_hidden_state\"]\n",
    "            else:\n",
    "                # TODO: This allows to support sentence-transformers models (sentence embedding), but is not validated.\n",
    "                last_hidden_state = next(iter(model_outputs.values()))\n",
    "\n",
    "        # converts output to namedtuple for pipelines post-processing\n",
    "        return BaseModelOutput(last_hidden_state=last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/bge_onnx/model_quantized.onnx\n",
      "False\n",
      "<onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions object at 0x7fdc4d966230>\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdc4d9481b0>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdc4d966230>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdc4d967170>]\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7fdc4d9481b0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-07-06 11:53:29.724101597 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 100 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2024-07-06 11:53:29.726186251 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-07-06 11:53:29.726195987 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface_optimum import OptimumEmbedding\n",
    "\n",
    "model = MyORTModelForFeatureExtraction.from_pretrained(\n",
    "    onnx_path, file_name=\"model_quantized.onnx\", provider=\"CUDAExecutionProvider\"\n",
    ")\n",
    "\n",
    "embed_model = OptimumEmbedding(folder_name=\"./tmp/bge_onnx\", model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "File \u001b[0;32m/mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    228\u001b[0m )\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m/mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:257\u001b[0m, in \u001b[0;36mBaseEmbedding.get_text_embedding\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    249\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    250\u001b[0m     EmbeddingStartEvent(\n\u001b[1;32m    251\u001b[0m         model_dict\u001b[38;5;241m=\u001b[39mmodel_dict,\n\u001b[1;32m    252\u001b[0m     )\n\u001b[1;32m    253\u001b[0m )\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    255\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mEMBEDDING, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mSERIALIZED: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict()}\n\u001b[1;32m    256\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m--> 257\u001b[0m     text_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_text_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m     event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[1;32m    260\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    261\u001b[0m             EventPayload\u001b[38;5;241m.\u001b[39mCHUNKS: [text],\n\u001b[1;32m    262\u001b[0m             EventPayload\u001b[38;5;241m.\u001b[39mEMBEDDINGS: [text_embedding],\n\u001b[1;32m    263\u001b[0m         }\n\u001b[1;32m    264\u001b[0m     )\n\u001b[1;32m    265\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    266\u001b[0m     EmbeddingEndEvent(\n\u001b[1;32m    267\u001b[0m         chunks\u001b[38;5;241m=\u001b[39m[text],\n\u001b[1;32m    268\u001b[0m         embeddings\u001b[38;5;241m=\u001b[39m[text_embedding],\n\u001b[1;32m    269\u001b[0m     )\n\u001b[1;32m    270\u001b[0m )\n",
      "File \u001b[0;32m/mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    228\u001b[0m )\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m/mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages/llama_index/embeddings/huggingface_optimum/base.py:172\u001b[0m, in \u001b[0;36mOptimumEmbedding._get_text_embedding\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get text embedding.\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m text \u001b[38;5;241m=\u001b[39m format_text(text, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_instruction)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages/llama_index/embeddings/huggingface_optimum/base.py:140\u001b[0m, in \u001b[0;36mOptimumEmbedding._embed\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Embed sentences.\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer(\n\u001b[1;32m    133\u001b[0m     sentences,\n\u001b[1;32m    134\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    138\u001b[0m )\n\u001b[0;32m--> 140\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoded_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    143\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cls_pooling(model_output)\n",
      "File \u001b[0;32m/mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages/optimum/modeling_base.py:95\u001b[0m, in \u001b[0;36mOptimizedModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 29\u001b[0m, in \u001b[0;36mMyORTModelForFeatureExtraction.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# run inference with binding & synchronize in case of multiple CUDA streams\u001b[39;00m\n\u001b[1;32m     28\u001b[0m io_binding\u001b[38;5;241m.\u001b[39msynchronize_inputs()\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_iobinding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio_binding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m io_binding\u001b[38;5;241m.\u001b[39msynchronize_outputs()\n\u001b[1;32m     32\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m output_buffers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m     33\u001b[0m     output_shapes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     34\u001b[0m )\n",
      "File \u001b[0;32m/mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:331\u001b[0m, in \u001b[0;36mSession.run_with_iobinding\u001b[0;34m(self, iobinding, run_options)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_with_iobinding\u001b[39m(\u001b[38;5;28mself\u001b[39m, iobinding, run_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    325\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03m    Compute the predictions.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m    :param iobinding: the iobinding object that has graph inputs/outputs bind.\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03m    :param run_options: See :class:`onnxruntime.RunOptions`.\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_iobinding\u001b[49m\u001b[43m(\u001b[49m\u001b[43miobinding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iobinding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(5000):\n",
    "    embeddings = embed_model.get_text_embedding(\"It is raining cats and dogs here!\")\n",
    "# embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
