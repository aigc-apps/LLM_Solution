{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Using cached huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting numpy<2.0,>=1.17 (from transformers)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Using cached tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
      "Using cached huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "Using cached PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
      "Using cached regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "Using cached safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n",
      "Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: urllib3, tqdm, safetensors, regex, pyyaml, numpy, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed certifi-2024.7.4 charset-normalizer-3.3.2 filelock-3.15.4 fsspec-2024.6.1 huggingface-hub-0.23.4 idna-3.7 numpy-1.26.4 pyyaml-6.0.1 regex-2024.5.15 requests-2.32.3 safetensors-0.4.3 tokenizers-0.19.1 tqdm-4.66.4 transformers-4.42.3 urllib3-2.2.2\n",
      "Collecting llama-index-embeddings-huggingface-optimum\n",
      "  Using cached llama_index_embeddings_huggingface_optimum-0.1.5-py3-none-any.whl.metadata (761 bytes)\n",
      "Collecting llama-index-core<0.11.0,>=0.10.1 (from llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached llama_index_core-0.10.52.post1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-embeddings-huggingface<0.2.0,>=0.1.3 (from llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached llama_index_embeddings_huggingface-0.1.5-py3-none-any.whl.metadata (755 bytes)\n",
      "Collecting optimum<2.0.0,>=1.16.2 (from optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached optimum-1.21.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (6.0.1)\n",
      "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached SQLAlchemy-2.0.31-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.6 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached aiohttp-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (2024.6.1)\n",
      "Collecting httpx (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting llama-cloud<0.0.7,>=0.0.6 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached llama_cloud-0.0.6-py3-none-any.whl.metadata (750 bytes)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (1.6.0)\n",
      "Collecting networkx>=3.0 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting nltk<4.0.0,>=3.8.1 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy<2.0.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (1.26.4)\n",
      "Collecting openai>=1.1.0 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached openai-1.35.10-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting pandas (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting pillow>=9.0.0 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: requests>=2.31.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached tenacity-8.4.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (4.12.2)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum) (0.23.4)\n",
      "Collecting torch<3.0.0,>=2.1.2 (from llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached torch-2.3.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.37.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum) (4.42.3)\n",
      "Collecting coloredlogs (from optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting sympy (from optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached sympy-1.12.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum) (24.1)\n",
      "Collecting datasets (from optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting onnx (from optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached onnx-1.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting onnxruntime (from optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached onnxruntime-1.18.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting timm (from optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached timm-1.0.7-py3-none-any.whl.metadata (47 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached multidict-6.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: filelock in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum) (3.15.4)\n",
      "Collecting minijinja>=1.0 (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached minijinja-2.0.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting pydantic>=1.10 (from llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting anyio (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: certifi in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (2024.7.4)\n",
      "Collecting httpcore==1.* (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: idna in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (3.7)\n",
      "Collecting sniffio (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting click (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (2024.5.15)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (2.2.2)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached greenlet-3.0.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting jinja2 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.3.1 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached triton-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers<5.0.0,>=4.37.0->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers<5.0.0,>=4.37.0->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum) (0.19.1)\n",
      "Collecting protobuf (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached protobuf-5.27.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached pyarrow-16.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xxhash (from datasets->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached xxhash-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting flatbuffers (from onnxruntime->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (2.9.0)\n",
      "Collecting pytz>=2020.1 (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting mpmath<1.4.0,>=1.1.0 (from sympy->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting torchvision (from timm->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached torchvision-0.18.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached pydantic_core-2.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (1.16.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Using cached llama_index_embeddings_huggingface_optimum-0.1.5-py3-none-any.whl (3.7 kB)\n",
      "Using cached llama_index_core-0.10.52.post1-py3-none-any.whl (15.4 MB)\n",
      "Using cached llama_index_embeddings_huggingface-0.1.5-py3-none-any.whl (7.8 kB)\n",
      "Using cached optimum-1.21.1-py3-none-any.whl (424 kB)\n",
      "Using cached aiohttp-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Using cached llama_cloud-0.0.6-py3-none-any.whl (130 kB)\n",
      "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached openai-1.35.10-py3-none-any.whl (328 kB)\n",
      "Using cached pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Using cached SQLAlchemy-2.0.31-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached tenacity-8.4.2-py3-none-any.whl (28 kB)\n",
      "Using cached tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Using cached torch-2.3.1-cp311-cp311-manylinux1_x86_64.whl (779.2 MB)\n",
      "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Using cached triton-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "Using cached fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "Using cached onnx-1.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
      "Using cached onnxruntime-1.18.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
      "Using cached pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "Using cached sympy-1.12.1-py3-none-any.whl (5.7 MB)\n",
      "Using cached timm-1.0.7-py3-none-any.whl (2.3 MB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)\n",
      "Using cached greenlet-3.0.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (620 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "Using cached minijinja-2.0.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (853 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached multidict-6.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached protobuf-5.27.2-cp38-abi3-manylinux2014_x86_64.whl (309 kB)\n",
      "Using cached pyarrow-16.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "Downloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.9/423.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pydantic_core-2.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (328 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached torchvision-0.18.1-cp311-cp311-manylinux1_x86_64.whl (7.0 MB)\n",
      "Using cached xxhash-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
      "Installing collected packages: sentencepiece, pytz, mpmath, flatbuffers, dirtyjson, xxhash, wrapt, tzdata, triton, tenacity, sympy, sniffio, pydantic-core, pyarrow-hotfix, pyarrow, protobuf, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, mypy-extensions, multidict, minijinja, marshmallow, MarkupSafe, joblib, humanfriendly, h11, greenlet, fsspec, frozenlist, distro, dill, click, attrs, annotated-types, yarl, typing-inspect, tiktoken, SQLAlchemy, pydantic, pandas, onnx, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nltk, multiprocess, jinja2, httpcore, deprecated, coloredlogs, anyio, aiosignal, onnxruntime, nvidia-cusolver-cu12, httpx, dataclasses-json, aiohttp, torch, openai, llama-cloud, torchvision, llama-index-core, datasets, timm, optimum, llama-index-embeddings-huggingface, llama-index-embeddings-huggingface-optimum\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.1\n",
      "    Uninstalling fsspec-2024.6.1:\n",
      "      Successfully uninstalled fsspec-2024.6.1\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install llama-index-embeddings-huggingface-optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optimum in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (1.21.1)\n",
      "Requirement already satisfied: coloredlogs in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from optimum) (15.0.1)\n",
      "Requirement already satisfied: sympy in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from optimum) (1.12.1)\n",
      "Requirement already satisfied: transformers<4.43.0,>=4.26.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (4.42.3)\n",
      "Requirement already satisfied: torch>=1.11 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from optimum) (2.3.1)\n",
      "Requirement already satisfied: packaging in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from optimum) (24.1)\n",
      "Requirement already satisfied: numpy<2.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from optimum) (1.26.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from optimum) (0.23.4)\n",
      "Requirement already satisfied: datasets in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from optimum) (2.20.0)\n",
      "Requirement already satisfied: filelock in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub>=0.8.0->optimum) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub>=0.8.0->optimum) (6.0.1)\n",
      "Requirement already satisfied: requests in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub>=0.8.0->optimum) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub>=0.8.0->optimum) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub>=0.8.0->optimum) (4.12.2)\n",
      "Requirement already satisfied: networkx in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (3.3)\n",
      "Requirement already satisfied: jinja2 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->optimum) (12.5.82)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (2024.5.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.19.1)\n",
      "Requirement already satisfied: protobuf in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (5.27.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.2.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from coloredlogs->optimum) (10.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from datasets->optimum) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from datasets->optimum) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from datasets->optimum) (0.3.8)\n",
      "Requirement already satisfied: pandas in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from datasets->optimum) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from datasets->optimum) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from datasets->optimum) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from datasets->optimum) (3.9.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from sympy->optimum) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from aiohttp->datasets->optimum) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from aiohttp->datasets->optimum) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from aiohttp->datasets->optimum) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from aiohttp->datasets->optimum) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from jinja2->torch>=1.11->optimum) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from pandas->datasets->optimum) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from pandas->datasets->optimum) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from pandas->datasets->optimum) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\n",
      "Requirement already satisfied: onnx in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (1.16.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from onnx) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from onnx) (5.27.2)\n",
      "Looking in indexes: https://pypi.org/simple, https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n",
      "Requirement already satisfied: onnxruntime-gpu in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (1.18.1)\n",
      "Requirement already satisfied: coloredlogs in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from onnxruntime-gpu) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from onnxruntime-gpu) (24.3.25)\n",
      "Requirement already satisfied: numpy<2.0,>=1.21.6 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from onnxruntime-gpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from onnxruntime-gpu) (24.1)\n",
      "Requirement already satisfied: protobuf in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from onnxruntime-gpu) (5.27.2)\n",
      "Requirement already satisfied: sympy in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from onnxruntime-gpu) (1.12.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from coloredlogs->onnxruntime-gpu) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from sympy->onnxruntime-gpu) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install optimum\n",
    "!pip install onnx\n",
    "!pip install onnxruntime-gpu --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/huggingface/sentence_transformers/bge-small-zh-v1.5'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "from transformers import Pipeline\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import os\n",
    "\n",
    "model_dir = \"/huggingface/sentence_transformers\"\n",
    "model_name = \"bge-small-zh-v1.5\"\n",
    "\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `from_transformers` is deprecated, and will be removed in optimum 2.0.  Use `export` instead\n",
      "Framework not specified. Using pt to export the model.\n",
      "Using the export variant default. Available variants are:\n",
      "    - default: The default ONNX variant.\n",
      "\n",
      "***** Exporting submodel 1/1: BertModel *****\n",
      "Using framework PyTorch: 2.3.1+cu121\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpjnjblbyp/model.onnx\n",
      "False\n",
      "<onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions object at 0x7f536f255b70>\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f536f255b70>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f536f255ab0>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f536ec3b4f0>]\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f536f255b70>]\n",
      "/tmp/tmpjnjblbyp/model.onnx\n",
      "False\n",
      "<onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions object at 0x7f536e2051b0>\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f536e2051b0>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f536decc230>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f536decfdb0>]\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f536e2051b0>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./tmp/bge_onnx/tokenizer_config.json',\n",
       " './tmp/bge_onnx/special_tokens_map.json',\n",
       " './tmp/bge_onnx/vocab.txt',\n",
       " './tmp/bge_onnx/added_tokens.json',\n",
       " './tmp/bge_onnx/tokenizer.json')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_path = \"./tmp/bge_onnx\"\n",
    "# load vanilla transformers and convert to onnx\n",
    "model = ORTModelForFeatureExtraction.from_pretrained(model_path, from_transformers=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# save onnx checkpoint and tokenizer\n",
    "model.save_pretrained(onnx_path)\n",
    "tokenizer.save_pretrained(onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "use_io_binding was set to False, setting it to True because it can provide a huge speedup on GPUs. It is possible to disable this feature manually by setting the use_io_binding attribute back to False.\n",
      "\u001b[0;93m2024-07-06 15:02:55.893775721 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 4 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpjnjblbyp/model.onnx\n",
      "False\n",
      "<onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions object at 0x7f536decff30>\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f536decff30>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f536e2051b0>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f54d2236930>]\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f536decff30>]\n",
      "tensor([ 0.0472,  0.0101,  0.0006, -0.0082, -0.0220])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-07-06 15:02:55.894689904 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-07-06 15:02:55.894697806 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# copied from the model card\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[\n",
    "        0\n",
    "    ]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = (\n",
    "        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    )\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "        input_mask_expanded.sum(1), min=1e-9\n",
    "    )\n",
    "\n",
    "\n",
    "class SentenceEmbeddingPipeline(Pipeline):\n",
    "    def _sanitize_parameters(self, **kwargs):\n",
    "        # we don't have any hyperameters to sanitize\n",
    "        preprocess_kwargs = {}\n",
    "        return preprocess_kwargs, {}, {}\n",
    "\n",
    "    def preprocess(self, inputs):\n",
    "        encoded_inputs = self.tokenizer(\n",
    "            inputs, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        return encoded_inputs\n",
    "\n",
    "    def _forward(self, model_inputs):\n",
    "        outputs = self.model(**model_inputs)\n",
    "        return {\"outputs\": outputs, \"attention_mask\": model_inputs[\"attention_mask\"]}\n",
    "\n",
    "    def postprocess(self, model_outputs):\n",
    "        # Perform pooling\n",
    "        sentence_embeddings = mean_pooling(\n",
    "            model_outputs[\"outputs\"], model_outputs[\"attention_mask\"]\n",
    "        )\n",
    "        # Normalize embeddings\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        return sentence_embeddings\n",
    "\n",
    "\n",
    "vanilla_emb = SentenceEmbeddingPipeline(model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# run inference\n",
    "pred = vanilla_emb(\"Could you assist me in finding my lost card?\")\n",
    "\n",
    "# print an excerpt from the sentence embedding\n",
    "print(pred[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages/optimum/onnxruntime/configuration.py:779: FutureWarning: disable_embed_layer_norm will be deprecated soon, use disable_embed_layer_norm_fusion instead, disable_embed_layer_norm_fusion is set to True.\n",
      "  warnings.warn(\n",
      "Optimizing model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/bge_onnx/model.onnx\n",
      "False\n",
      "<onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions object at 0x7f532c2a96f0>\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f532c2a96f0>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f532c2a9ff0>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f532c2a9730>]\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f532c2a96f0>]\n",
      "tmp/bge_onnx/model.onnx\n",
      "False\n",
      "<onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions object at 0x7f532c390a30>\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f532c393a30>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f53b6024870>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f5368374270>]\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f532c393a30>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-07-06 15:03:00.232955208 [W:onnxruntime:, inference_session.cc:1978 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.\u001b[m\n",
      "Configuration saved in tmp/bge_onnx/ort_config.json\n",
      "Optimized model saved at: tmp/bge_onnx (external data format: False; saved all tensor to one file: True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('tmp/bge_onnx')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTOptimizer\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig\n",
    "\n",
    "model = ORTModelForFeatureExtraction.from_pretrained(onnx_path, file_name=\"model.onnx\")\n",
    "\n",
    "# create ORTOptimizer and define optimization configuration\n",
    "optimizer = ORTOptimizer.from_pretrained(model)\n",
    "optimization_config = OptimizationConfig(\n",
    "    optimization_level=99\n",
    ")  # enable all optimizations\n",
    "\n",
    "# apply the optimization configuration to the model\n",
    "optimizer.optimize(\n",
    "    save_dir=onnx_path,\n",
    "    optimization_config=optimization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "use_io_binding was set to False, setting it to True because it can provide a huge speedup on GPUs. It is possible to disable this feature manually by setting the use_io_binding attribute back to False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/bge_onnx/model_optimized.onnx\n",
      "False\n",
      "<onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions object at 0x7f536deb7d70>\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f536deb7d70>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f532c2ab930>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f532c2aa270>]\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f536deb7d70>]\n",
      "tmp/bge_onnx/model_optimized.onnx\n",
      "False\n",
      "<onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions object at 0x7f532c2aae30>\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f532c2aae30>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f532c2a95b0>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f532c2ab6b0>]\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f532c2aae30>]\n",
      "tensor([ 0.0472,  0.0101,  0.0006, -0.0082, -0.0220])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-07-06 15:03:08.539916008 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 4 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2024-07-06 15:03:08.540708609 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-07-06 15:03:08.540715910 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
    "\n",
    "# load optimized model\n",
    "model = ORTModelForFeatureExtraction.from_pretrained(\n",
    "    onnx_path, file_name=\"model_optimized.onnx\"\n",
    ")\n",
    "\n",
    "# create optimized pipeline\n",
    "optimized_emb = SentenceEmbeddingPipeline(model=model, tokenizer=tokenizer, device=0)\n",
    "pred = optimized_emb(\"Could you assist me in finding my lost card?\")\n",
    "print(pred[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply ORTQuantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating dynamic quantizer: QOperator (mode: IntegerOps, schema: u8/s8, channel-wise: False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/bge_onnx/model_optimized.onnx\n",
      "False\n",
      "<onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions object at 0x7f536deb7d70>\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f536deb7d70>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f532475ae70>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f532475a470>]\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f536deb7d70>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing model...\n",
      "Saving quantized model at: tmp/bge_onnx (external data format: False)\n",
      "Configuration saved in tmp/bge_onnx/ort_config.json\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "\n",
    "import onnx\n",
    "\n",
    "model = ORTModelForFeatureExtraction.from_pretrained(\n",
    "    onnx_path, file_name=\"model_optimized.onnx\"\n",
    ")\n",
    "\n",
    "# create ORTQuantizer and define quantization configuration\n",
    "dynamic_quantizer = ORTQuantizer.from_pretrained(model)\n",
    "dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)\n",
    "\n",
    "# apply the quantization configuration to the model\n",
    "model_quantized_path = dynamic_quantizer.quantize(\n",
    "    save_dir=onnx_path,\n",
    "    quantization_config=dqconfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import TYPE_CHECKING, Any, Dict, List, Optional, Set, Tuple, Union\n",
    "from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "\n",
    "\n",
    "class MyORTModelForFeatureExtraction(ORTModelForFeatureExtraction):\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[Union[torch.Tensor, np.ndarray]] = None,\n",
    "        attention_mask: Optional[Union[torch.Tensor, np.ndarray]] = None,\n",
    "        token_type_ids: Optional[Union[torch.Tensor, np.ndarray]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        use_torch = isinstance(input_ids, torch.Tensor)\n",
    "        self.raise_on_numpy_input_io_binding(use_torch)\n",
    "\n",
    "        if self.device.type == \"cuda\" and self.use_io_binding:\n",
    "            io_binding, output_shapes, output_buffers = self.prepare_io_binding(\n",
    "                input_ids,\n",
    "                attention_mask,\n",
    "                token_type_ids,\n",
    "                ordered_input_names=self._ordered_input_names,\n",
    "            )\n",
    "\n",
    "            # run inference with binding & synchronize in case of multiple CUDA streams\n",
    "            io_binding.synchronize_inputs()\n",
    "            self.model.run_with_iobinding(io_binding)\n",
    "            io_binding.synchronize_outputs()\n",
    "\n",
    "            last_hidden_state = output_buffers[\"last_hidden_state\"].view(\n",
    "                output_shapes[\"last_hidden_state\"]\n",
    "            )\n",
    "        else:\n",
    "            model_inputs = {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"token_type_ids\": token_type_ids,\n",
    "            }\n",
    "\n",
    "            onnx_inputs = self._prepare_onnx_inputs(use_torch, **model_inputs)\n",
    "            onnx_outputs = self.model.run(None, onnx_inputs)\n",
    "            model_outputs = self._prepare_onnx_outputs(use_torch, *onnx_outputs)\n",
    "\n",
    "            if \"last_hidden_state\" in self.output_names:\n",
    "                last_hidden_state = model_outputs[\"last_hidden_state\"]\n",
    "            else:\n",
    "                # TODO: This allows to support sentence-transformers models (sentence embedding), but is not validated.\n",
    "                last_hidden_state = next(iter(model_outputs.values()))\n",
    "\n",
    "        # converts output to namedtuple for pipelines post-processing\n",
    "        return BaseModelOutput(last_hidden_state=last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The ONNX file model_optimized_quantized.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/bge_onnx/model_optimized_quantized.onnx\n",
      "False\n",
      "<onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions object at 0x7f53ba042e30>\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f53ba042e30>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f52501f7c30>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f52ec6b9330>]\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f53ba042e30>]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface_optimum import OptimumEmbedding\n",
    "\n",
    "model = MyORTModelForFeatureExtraction.from_pretrained(\n",
    "    onnx_path, file_name=\"model_optimized_quantized.onnx\"\n",
    ")\n",
    "\n",
    "embed_model = OptimumEmbedding(folder_name=\"./tmp/bge_onnx\", model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 30s, sys: 15.5 ms, total: 2min 30s\n",
      "Wall time: 9.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(5000):\n",
    "    embeddings = embed_model.get_text_embedding(\"It is raining cats and dogs here!\")\n",
    "# embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/bge_onnx/model_optimized.onnx\n",
      "False\n",
      "<onnxruntime.capi.onnxruntime_pybind11_state.SessionOptions object at 0x7f52501b9b30>\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f52501b9b30>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f52501b7870>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f525061c6f0>]\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7f52501e9bb0>]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface_optimum import OptimumEmbedding\n",
    "\n",
    "model = MyORTModelForFeatureExtraction.from_pretrained(\n",
    "    onnx_path, file_name=\"model_optimized.onnx\"\n",
    ")\n",
    "\n",
    "embed_model = OptimumEmbedding(folder_name=\"./tmp/bge_onnx\", model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 50s, sys: 20.6 ms, total: 2min 50s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(5000):\n",
    "    embeddings = embed_model.get_text_embedding(\"It is raining cats and dogs here!\")\n",
    "# embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
