{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Using cached huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting numpy<2.0,>=1.17 (from transformers)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Using cached tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
      "Using cached huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "Using cached PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
      "Using cached regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "Using cached safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n",
      "Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: urllib3, tqdm, safetensors, regex, pyyaml, numpy, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed certifi-2024.7.4 charset-normalizer-3.3.2 filelock-3.15.4 fsspec-2024.6.1 huggingface-hub-0.23.4 idna-3.7 numpy-1.26.4 pyyaml-6.0.1 regex-2024.5.15 requests-2.32.3 safetensors-0.4.3 tokenizers-0.19.1 tqdm-4.66.4 transformers-4.42.3 urllib3-2.2.2\n",
      "Collecting llama-index-embeddings-huggingface-optimum\n",
      "  Using cached llama_index_embeddings_huggingface_optimum-0.1.5-py3-none-any.whl.metadata (761 bytes)\n",
      "Collecting llama-index-core<0.11.0,>=0.10.1 (from llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached llama_index_core-0.10.52.post1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-embeddings-huggingface<0.2.0,>=0.1.3 (from llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached llama_index_embeddings_huggingface-0.1.5-py3-none-any.whl.metadata (755 bytes)\n",
      "Collecting optimum<2.0.0,>=1.16.2 (from optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached optimum-1.21.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (6.0.1)\n",
      "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached SQLAlchemy-2.0.31-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.6 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached aiohttp-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (2024.6.1)\n",
      "Collecting httpx (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting llama-cloud<0.0.7,>=0.0.6 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached llama_cloud-0.0.6-py3-none-any.whl.metadata (750 bytes)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (1.6.0)\n",
      "Collecting networkx>=3.0 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting nltk<4.0.0,>=3.8.1 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy<2.0.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (1.26.4)\n",
      "Collecting openai>=1.1.0 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached openai-1.35.10-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting pandas (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting pillow>=9.0.0 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: requests>=2.31.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached tenacity-8.4.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (4.12.2)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum) (0.23.4)\n",
      "Collecting torch<3.0.0,>=2.1.2 (from llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached torch-2.3.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.37.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum) (4.42.3)\n",
      "Collecting coloredlogs (from optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting sympy (from optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached sympy-1.12.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum) (24.1)\n",
      "Collecting datasets (from optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting onnx (from optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached onnx-1.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting onnxruntime (from optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached onnxruntime-1.18.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting timm (from optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached timm-1.0.7-py3-none-any.whl.metadata (47 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached multidict-6.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: filelock in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum) (3.15.4)\n",
      "Collecting minijinja>=1.0 (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached minijinja-2.0.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting pydantic>=1.10 (from llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting anyio (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: certifi in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (2024.7.4)\n",
      "Collecting httpcore==1.* (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: idna in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (3.7)\n",
      "Collecting sniffio (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting click (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (2024.5.15)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (2.2.2)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached greenlet-3.0.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting jinja2 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.3.1 (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached triton-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers<5.0.0,>=4.37.0->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers<5.0.0,>=4.37.0->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum) (0.19.1)\n",
      "Collecting protobuf (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached protobuf-5.27.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached pyarrow-16.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xxhash (from datasets->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached xxhash-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting flatbuffers (from onnxruntime->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (2.9.0)\n",
      "Collecting pytz>=2020.1 (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting mpmath<1.4.0,>=1.1.0 (from sympy->optimum<2.0.0,>=1.16.2->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting torchvision (from timm->optimum[exporters]<2.0.0,>=1.16.2->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached torchvision-0.18.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached pydantic_core-2.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface-optimum) (1.16.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface<0.2.0,>=0.1.3->llama-index-embeddings-huggingface-optimum)\n",
      "  Using cached MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Using cached llama_index_embeddings_huggingface_optimum-0.1.5-py3-none-any.whl (3.7 kB)\n",
      "Using cached llama_index_core-0.10.52.post1-py3-none-any.whl (15.4 MB)\n",
      "Using cached llama_index_embeddings_huggingface-0.1.5-py3-none-any.whl (7.8 kB)\n",
      "Using cached optimum-1.21.1-py3-none-any.whl (424 kB)\n",
      "Using cached aiohttp-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Using cached llama_cloud-0.0.6-py3-none-any.whl (130 kB)\n",
      "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached openai-1.35.10-py3-none-any.whl (328 kB)\n",
      "Using cached pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Using cached SQLAlchemy-2.0.31-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached tenacity-8.4.2-py3-none-any.whl (28 kB)\n",
      "Using cached tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Using cached torch-2.3.1-cp311-cp311-manylinux1_x86_64.whl (779.2 MB)\n",
      "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Using cached triton-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "Using cached fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "Using cached onnx-1.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
      "Using cached onnxruntime-1.18.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
      "Using cached pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "Using cached sympy-1.12.1-py3-none-any.whl (5.7 MB)\n",
      "Using cached timm-1.0.7-py3-none-any.whl (2.3 MB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)\n",
      "Using cached greenlet-3.0.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (620 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "Using cached minijinja-2.0.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (853 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached multidict-6.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached protobuf-5.27.2-cp38-abi3-manylinux2014_x86_64.whl (309 kB)\n",
      "Using cached pyarrow-16.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "Downloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.9/423.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pydantic_core-2.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (328 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached torchvision-0.18.1-cp311-cp311-manylinux1_x86_64.whl (7.0 MB)\n",
      "Using cached xxhash-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
      "Installing collected packages: sentencepiece, pytz, mpmath, flatbuffers, dirtyjson, xxhash, wrapt, tzdata, triton, tenacity, sympy, sniffio, pydantic-core, pyarrow-hotfix, pyarrow, protobuf, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, mypy-extensions, multidict, minijinja, marshmallow, MarkupSafe, joblib, humanfriendly, h11, greenlet, fsspec, frozenlist, distro, dill, click, attrs, annotated-types, yarl, typing-inspect, tiktoken, SQLAlchemy, pydantic, pandas, onnx, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nltk, multiprocess, jinja2, httpcore, deprecated, coloredlogs, anyio, aiosignal, onnxruntime, nvidia-cusolver-cu12, httpx, dataclasses-json, aiohttp, torch, openai, llama-cloud, torchvision, llama-index-core, datasets, timm, optimum, llama-index-embeddings-huggingface, llama-index-embeddings-huggingface-optimum\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.1\n",
      "    Uninstalling fsspec-2024.6.1:\n",
      "      Successfully uninstalled fsspec-2024.6.1\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install llama-index-embeddings-huggingface-optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optimum in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (1.21.1)\n",
      "Requirement already satisfied: coloredlogs in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from optimum) (15.0.1)\n",
      "Requirement already satisfied: sympy in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from optimum) (1.12.1)\n",
      "Requirement already satisfied: transformers<4.43.0,>=4.26.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (4.42.3)\n",
      "Requirement already satisfied: torch>=1.11 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from optimum) (2.3.1)\n",
      "Requirement already satisfied: packaging in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from optimum) (24.1)\n",
      "Requirement already satisfied: numpy<2.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from optimum) (1.26.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from optimum) (0.23.4)\n",
      "Requirement already satisfied: datasets in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from optimum) (2.20.0)\n",
      "Requirement already satisfied: filelock in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub>=0.8.0->optimum) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub>=0.8.0->optimum) (6.0.1)\n",
      "Requirement already satisfied: requests in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub>=0.8.0->optimum) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub>=0.8.0->optimum) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from huggingface-hub>=0.8.0->optimum) (4.12.2)\n",
      "Requirement already satisfied: networkx in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (3.3)\n",
      "Requirement already satisfied: jinja2 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from torch>=1.11->optimum) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->optimum) (12.5.82)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (2024.5.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.19.1)\n",
      "Requirement already satisfied: protobuf in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (5.27.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.2.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from coloredlogs->optimum) (10.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from datasets->optimum) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from datasets->optimum) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from datasets->optimum) (0.3.8)\n",
      "Requirement already satisfied: pandas in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from datasets->optimum) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from datasets->optimum) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from datasets->optimum) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from datasets->optimum) (3.9.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from sympy->optimum) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from aiohttp->datasets->optimum) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from aiohttp->datasets->optimum) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from aiohttp->datasets->optimum) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from aiohttp->datasets->optimum) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from jinja2->torch>=1.11->optimum) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from pandas->datasets->optimum) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from pandas->datasets->optimum) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from pandas->datasets->optimum) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\n",
      "Requirement already satisfied: onnx in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (1.16.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from onnx) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from onnx) (5.27.2)\n",
      "Looking in indexes: https://pypi.org/simple, https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n",
      "Requirement already satisfied: onnxruntime-gpu in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (1.18.1)\n",
      "Requirement already satisfied: coloredlogs in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from onnxruntime-gpu) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from onnxruntime-gpu) (24.3.25)\n",
      "Requirement already satisfied: numpy<2.0,>=1.21.6 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from onnxruntime-gpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from onnxruntime-gpu) (24.1)\n",
      "Requirement already satisfied: protobuf in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from onnxruntime-gpu) (5.27.2)\n",
      "Requirement already satisfied: sympy in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from onnxruntime-gpu) (1.12.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from coloredlogs->onnxruntime-gpu) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /mnt/llm/feiyue/miniconda3/envs/onnx/lib/python3.11/site-packages (from sympy->onnxruntime-gpu) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install optimum\n",
    "!pip install onnx\n",
    "!pip install onnxruntime-gpu --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/huggingface/sentence_transformers/bge-small-zh-v1.5'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_dir = \"/huggingface/sentence_transformers\"\n",
    "model_name = \"bge-small-zh-v1.5\"\n",
    "\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export the model.\n",
      "Using the export variant default. Available variants are:\n",
      "    - default: The default ONNX variant.\n",
      "\n",
      "***** Exporting submodel 1/1: BertModel *****\n",
      "Using framework PyTorch: 2.3.1+cu121\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved optimum model to ./tmp/bge_onnx. Use it with `embed_model = OptimumEmbedding(folder_name='./tmp/bge_onnx')`.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface_optimum import OptimumEmbedding\n",
    "\n",
    "OptimumEmbedding.create_and_save_optimum_model(model_path, \"./tmp/bge_onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import TYPE_CHECKING, Any, Dict, List, Optional, Set, Tuple, Union\n",
    "from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "\n",
    "\n",
    "class MyORTModelForFeatureExtraction(ORTModelForFeatureExtraction):\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[Union[torch.Tensor, np.ndarray]] = None,\n",
    "        attention_mask: Optional[Union[torch.Tensor, np.ndarray]] = None,\n",
    "        token_type_ids: Optional[Union[torch.Tensor, np.ndarray]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        use_torch = isinstance(input_ids, torch.Tensor)\n",
    "        self.raise_on_numpy_input_io_binding(use_torch)\n",
    "\n",
    "        if self.device.type == \"cuda\" and self.use_io_binding:\n",
    "            io_binding, output_shapes, output_buffers = self.prepare_io_binding(\n",
    "                input_ids,\n",
    "                attention_mask,\n",
    "                token_type_ids,\n",
    "                ordered_input_names=self._ordered_input_names,\n",
    "            )\n",
    "\n",
    "            # run inference with binding & synchronize in case of multiple CUDA streams\n",
    "            io_binding.synchronize_inputs()\n",
    "            self.model.run_with_iobinding(io_binding)\n",
    "            io_binding.synchronize_outputs()\n",
    "\n",
    "            last_hidden_state = output_buffers[\"last_hidden_state\"].view(\n",
    "                output_shapes[\"last_hidden_state\"]\n",
    "            )\n",
    "        else:\n",
    "            model_inputs = {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"token_type_ids\": token_type_ids,\n",
    "            }\n",
    "\n",
    "            onnx_inputs = self._prepare_onnx_inputs(use_torch, **model_inputs)\n",
    "            onnx_outputs = self.model.run(None, onnx_inputs)\n",
    "            model_outputs = self._prepare_onnx_outputs(use_torch, *onnx_outputs)\n",
    "\n",
    "            if \"last_hidden_state\" in self.output_names:\n",
    "                last_hidden_state = model_outputs[\"last_hidden_state\"]\n",
    "            else:\n",
    "                # TODO: This allows to support sentence-transformers models (sentence embedding), but is not validated.\n",
    "                last_hidden_state = next(iter(model_outputs.values()))\n",
    "\n",
    "        # converts output to namedtuple for pipelines post-processing\n",
    "        return BaseModelOutput(last_hidden_state=last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-07-04 13:57:07.844526736 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 4 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2024-07-04 13:57:07.845440330 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-07-04 13:57:07.845449429 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "model = MyORTModelForFeatureExtraction.from_pretrained(\n",
    "    \"./tmp/bge_onnx\", provider=\"CUDAExecutionProvider\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface_optimum import OptimumEmbedding\n",
    "\n",
    "embed_model = OptimumEmbedding(folder_name=\"./tmp/bge_onnx\", model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.04 s, sys: 0 ns, total: 8.04 s\n",
      "Wall time: 8.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(5000):\n",
    "    embeddings = embed_model.get_text_embedding(\"It is raining cats and dogs here!\")\n",
    "# embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
